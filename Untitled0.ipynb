{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HT2bZvmULIHO"
      },
      "source": [
        "# **A. Introduction**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O3Jmy8pi2S7y"
      },
      "source": [
        "# **B. Approach**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l9mwxdvX2wvK"
      },
      "source": [
        "\n",
        "\n",
        "## **1.   Setup Framework**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VPHuJlbl3Hg-"
      },
      "source": [
        "Before model training, it is essential to set up a robust evaluation framework to ensure that the model's performance is fairly and accurately assessed. This involves splitting the data into training, validation, and testing datasets. The training set is used to train the model, the validation set is used to tune hyperparameters, and the test set provides an unbiased evaluation of the model's performance.\n",
        "\n",
        "**Rationale:** A typical split of 70% training, 15% validation, and 15% testing ensures sufficient data for training while preserving enough data to validate the model and assess generalization performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CeHfsOjR4LJg"
      },
      "outputs": [],
      "source": [
        "%pip install -q -U keras-tuner"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mg5znjZBzuvp"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import zipfile\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import keras_tuner as kt\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import seaborn as sns\n",
        "import imageio\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from pathlib import Path\n",
        "from time import strftime\n",
        "import uuid\n",
        "from tensorflow.keras import layers, models, optimizers\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.applications import Xception, MobileNetV2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9HC-qWGWztyD",
        "outputId": "2dbef011-3800-456d-fc41-2d5a98e07aaa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The tensorboard extension is already loaded. To reload it, use:\n",
            "  %reload_ext tensorboard\n"
          ]
        }
      ],
      "source": [
        "%load_ext tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mcp97DNRzw9p"
      },
      "outputs": [],
      "source": [
        "%reload_ext tensorboard"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vGwj9U68bHgt"
      },
      "source": [
        "## **2. Connect data file**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LX4Cf50OPZxt"
      },
      "source": [
        "*   ***Mounting Google Drive:*** The code uses drive.mount('/content/drive') to mount Google Drive into the Colab environment, enabling access to files stored in the cloud. This is essential for handling large datasets and persistent storage in a seamless and reliable way, commonly used in Colab for large-scale data analysis.\n",
        "\n",
        "*   ***Setting the Data Path:*** The data_path variable specifies the location of the dataset in Google Drive. This makes the code more flexible and easier to maintain, as changes to the data location can be made by simply updating this variable.\n",
        "\n",
        "*   ***Changing the Working Directory:*** Using os.chdir(data_path), the working directory is set to the specified data path. This allows for easier file handling by making all subsequent operations relative to this directory, reducing the need for full path specifications and minimizing errors.\n",
        "\n",
        "*   ***Listing the Contents of the Current Directory:*** The command os.listdir() lists the files in the current directory, confirming that the working directory is set correctly and that the necessary data files are present. This step acts as a checkpoint before proceeding with further data processing.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O2OxOdpsbV-h",
        "outputId": "e2ab8dca-65be-4cde-cc5c-96397b34db7d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "['s1234567_predictions.csv',\n",
              " 'future_data_2024.csv',\n",
              " 'train_data_2024.csv',\n",
              " 'Images.zip',\n",
              " '__MACOSX',\n",
              " 'Images',\n",
              " 'my_logs',\n",
              " 'models',\n",
              " 'class_mapping.csv']"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "data_path = '/content/drive/MyDrive/COSC2779/s3927777-Assignment1/A1_2024_data'\n",
        "\n",
        "# Change the working directory to data_path\n",
        "os.chdir(data_path)\n",
        "\n",
        "# Print out the current directory to validate if it is the correct directory\n",
        "os.listdir()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bmqPhmgnq8WO",
        "outputId": "f0836c48-1a74-4e0e-adad-72bc48fbd5b0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "replace Images/Img_5811.jpg? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ]
        }
      ],
      "source": [
        "# Unzip image file (only if the image file hasn't been unzipped)\n",
        "!unzip -q Images.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Q78GTzPbZY4"
      },
      "source": [
        "## **3. Load CSV files**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BmhVEWKxRosY"
      },
      "source": [
        "The code loads two CSV files into Pandas DataFrames: train_data for training and future_data for future predictions. Besides, the 'FilePath' column is added to both DataFrames to store the full path to each image file. The paths are constructed by joining a common directory (image_dir) with the individual filenames from the FileName column. By that way, constructing full paths for each image ensures that the images can be easily accessed later, facilitating image loading for training and prediction. This approach centralizes file handling and ensures consistency, reducing the likelihood of errors when accessing image files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "olXu2c11bb-J"
      },
      "outputs": [],
      "source": [
        "train_data = pd.read_csv('train_data_2024.csv')\n",
        "future_data = pd.read_csv('future_data_2024.csv')\n",
        "\n",
        "image_dir = './Images/'\n",
        "# Get all file paths of the images\n",
        "train_data['FilePath'] = train_data['FileName'].apply(lambda x: os.path.join(image_dir, x))\n",
        "future_data['FilePath'] = future_data['FileName'].apply(lambda x: os.path.join(image_dir, x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vWEr1-bgpNcB"
      },
      "outputs": [],
      "source": [
        "future_data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4vOI-ZKlpRql"
      },
      "outputs": [],
      "source": [
        "train_data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qqN0r3dHlwsk"
      },
      "outputs": [],
      "source": [
        "RANDOM_STATE = 42\n",
        "tf.random.set_seed(RANDOM_STATE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mvIrk8S8bftH"
      },
      "source": [
        "## **4. Preprocess the data**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ou-PY1N74xCQ"
      },
      "source": [
        "By examining the number of distinct values in the 'Class' column, you ensure that the dataset is correctly labeled for training. If the result deviates from 40, it may indicate missing or extra classes that could impact model performance.\n",
        "\n",
        "In addition to verifying the number of classes, it's important to note that the development data includes three labels: 'Class', 'MoreThanOnePerson', and 'HighLevelCategory', with 'FileName' being excluded from predictions. For the primary model, you only need to predict the 'Class' and 'MoreThanOnePerson' labels. The 'HighLevelCategory' label, although not necessary for the main prediction task, could be useful as an auxiliary output layer. Integrating 'HighLevelCategory' in this way might help in normalizing the model or improving its performance by providing additional contextual information."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FJAQO9cLrMwz"
      },
      "outputs": [],
      "source": [
        "# Verify if the quantity of classes aligns with the specifications by confirming that there are indeed 40 classes in total\n",
        "train_data['Class'].nunique()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jED6Yjnb5dDM"
      },
      "source": [
        "By counting the number of non-null entries in the 'FileName' column, this line provides the total count of data points available for training. This count is essential for understanding the size of your dataset and ensuring that you have enough instances to build and evaluate your model effectively."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "00O3Jht5rNnr"
      },
      "outputs": [],
      "source": [
        "# Check how many instances\n",
        "train_data['FileName'].count()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gAkudRL95f83"
      },
      "source": [
        "It is also important to note that the future_data does not contain any labels, rendering it unsuitable for evaluation purposes. As a result, the data from the train_data_2024.csv file will be used and split into training, testing, and validation datasets. Furthermore, it has been confirmed that the train data contains the correct number of unique classes, aligning with the model's requirements and ensuring that the dataset is appropriately structured for training and evaluation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iBbGmVWbz9Bf"
      },
      "source": [
        "## **5. Validate Images**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7B3eHnVGz_Ty"
      },
      "outputs": [],
      "source": [
        "# Check if any Filename overlap between 2 data file\n",
        "train_data['FileName'].isin(future_data['FileName']).sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y6jaqCaY6WU6"
      },
      "source": [
        "This count helps to detect potential data leakage, which occurs if the same images are present in both datasets. Ensuring that there are no overlapping filenames is crucial for maintaining the integrity of your training and evaluation processes, as it prevents the possibility of the model being inadvertently trained or tested on duplicate data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U3_k1OPeKhFY"
      },
      "outputs": [],
      "source": [
        "# Get the size of an image given its filepath\n",
        "def image_dimensions(filepath):\n",
        "    image = imageio.imread(filepath)\n",
        "    width, height = image.shape[1], image.shape[0]\n",
        "    return width, height\n",
        "\n",
        "# Check the sizes of all images listed in the 'FileName' column of a DataFrame\n",
        "def get_all_image_sizes(data):\n",
        "    # Initialize an empty list to store the image sizes\n",
        "    image_sizes = []\n",
        "\n",
        "    # Loop over each filename in the 'FileName' column of the data\n",
        "    for filename in data['FileName']:\n",
        "        # Construct the full path to the image file\n",
        "        image_path = os.path.join('Images', filename)\n",
        "        print(image_path)\n",
        "\n",
        "        # Get the dimensions of the image\n",
        "        dimensions = image_dimensions(image_path)\n",
        "\n",
        "        # Append the dimensions to the list\n",
        "        image_sizes.append(dimensions)\n",
        "\n",
        "    return image_sizes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CLmJGz-7LhM5"
      },
      "outputs": [],
      "source": [
        "sizes = get_all_image_sizes(train_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FvUubQm06tmj"
      },
      "source": [
        "The code snippet 'image_dimensions(filepath)' defines a function to retrieve the dimensions of an image given its file path. By using 'imageio.imread(filepath)', the function loads the image, and image.shape provides its width and height. The function returns these dimensions, which are essential for analyzing and ensuring consistency in image sizes within the dataset.\n",
        "\n",
        "To check the sizes of all images listed in the 'FileName' column of a DataFrame, the function 'get_all_image_sizes(data)' is used. This function iterates over each filename, constructs the full path to the image using 'os.path.join('Images', filename)', and retrieves the image dimensions through the image_dimensions function. The sizes are then collected in a list. This approach allows you to systematically analyze the dimensions of all images in the dataset, which is crucial for preprocessing and ensuring that all images conform to the required specifications for model training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G5hjmhNjLzpN"
      },
      "outputs": [],
      "source": [
        "# Plot the sizes\n",
        "plt.hist(sizes, bins=10)\n",
        "plt.xlabel('Image Size')\n",
        "plt.ylabel('Occurences')\n",
        "plt.title('Distribution')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Frd__mlN7LAV"
      },
      "source": [
        "The plot illustrates the distribution of image sizes within your dataset. The following key points can be concluded:\n",
        "\n",
        "\n",
        "*   **Sparse Distribution:** The histogram shows that the image sizes are sparsely distributed across different values. This is indicated by the tall, thin lines corresponding to individual image sizes, suggesting that there are not many images of the same size.\n",
        "\n",
        "*   **No Common Image Size:** Unlike typical datasets where a peak or cluster might indicate a common or predominant image size, this dataset lacks such clustering. This could suggest significant variation in the dimensions of images.\n",
        "\n",
        "*   **Implications for Preprocessing:**\n",
        "\n",
        "  *   ***Resizing Required:*** Given the variety in image sizes, it's likely that resizing images to a uniform dimension will be necessary before feeding them into a deep learning model, especially we are planning to use a pre-trained Convolutional Neural Network (CNN).\n",
        "\n",
        "  *   ***Memory and Performance Consideration:*** The variation in image sizes might also impact memory usage and processing time, making it even more important to standardize image dimensions.\n",
        "\n",
        "\n",
        "*   **Outliers:** There might be outliers present with very large or very small dimensions compared to the rest. These should be handled carefully, as they can skew model training if not properly addressed.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oumBAqotMKUZ"
      },
      "source": [
        "## **6. Check the Imbalance of Data**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gKalpKyU8Rz1"
      },
      "source": [
        "The distribution of classes was considered, and techniques like class weighting or oversampling could be considered if significant imbalance is found."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zu1ibPsh8uBp"
      },
      "source": [
        "To analyze the distribution of occurrences within the dataset, the code snippet 'grouped_counts = train_data.groupby(['Class', 'MoreThanOnePerson']).size().reset_index(name='Count')' groups the data by the 'Class' and 'MoreThanOnePerson' columns, then counts the number of occurrences for each combination. This grouping is useful for understanding how many instances fall into each class and whether they contain more than one person.\n",
        "\n",
        "Following this, 'the pivot_table = grouped_counts.pivot(index='Class', columns='MoreThanOnePerson', values='Count').fillna(0)' reshapes the grouped data into a format suitable for plotting. By pivoting the table, you can create a stacked bar chart to visually represent the distribution of 'MoreThanOnePerson' within each 'Class'. The plotting code 'ax = pivot_table.plot(kind='bar', stacked=True, figsize=(10, 6))' generates this chart, with 'ax.set_title()', 'ax.set_xlabel()', 'ax.set_ylabel()', and 'ax.set_xticklabels()' enhancing readability and presentation. The final result is a clear visualization showing how many images in each class involve more than one person, facilitating a better understanding of the dataset's composition."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8wvbjw_iMJhm"
      },
      "outputs": [],
      "source": [
        "# Group the data and count occurrences\n",
        "grouped_counts = train_data.groupby(['Class', 'MoreThanOnePerson']).size().reset_index(name='Count')\n",
        "\n",
        "# Pivot the table to make it suitable for plotting\n",
        "pivot_table = grouped_counts.pivot(index='Class', columns='MoreThanOnePerson', values='Count').fillna(0)\n",
        "\n",
        "# Plot the data using the `plot` method\n",
        "ax = pivot_table.plot(kind='bar', stacked=True, figsize=(10, 6))  # Adjust figsize for better visualization\n",
        "ax.set_title('Distribution of MoreThanOnePerson for Each Class')\n",
        "ax.set_xlabel('Class')\n",
        "ax.set_ylabel('Count')\n",
        "ax.set_xticklabels(pivot_table.index, rotation=90)  # Rotate x-axis labels for readability\n",
        "ax.legend(title='MoreThanOnePerson')\n",
        "plt.tight_layout()  # Adjust layout for better spacing\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "62cFz-7bNQhk"
      },
      "outputs": [],
      "source": [
        "# Count occurrences of each category in 'HighLevelCategory'\n",
        "category_counts = train_data['HighLevelCategory'].value_counts()\n",
        "\n",
        "# Plot the bar chart using the `plot` method\n",
        "ax = category_counts.plot(kind='bar', figsize=(8, 5))  # Adjust figsize for visualization\n",
        "ax.set_title('Value Counts of HighLevelCategory')\n",
        "ax.set_xlabel('HighLevelCategory')\n",
        "ax.set_ylabel('Count')\n",
        "plt.xticks(rotation=45)  # Rotate x-axis labels for better readability if needed\n",
        "plt.tight_layout()  # Adjust layout to prevent overlap\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nd2vme8CAGkU"
      },
      "source": [
        "**Conclusion of the plots**\n",
        "\n",
        "The imbalance in the MoreThanOnePerson occurrences and the uneven distribution of classes and categories might affect the model’s ability to generalize across different scenarios. This imbalance could be due to varying scene complexities or contexts within each class. To address these issues, several strategies might be employed:\n",
        "\n",
        "*   **Data Augmentation:** Implementing data augmentation techniques can help balance the occurrences within each class while maintaining a balanced distribution of the MoreThanOnePerson label.\n",
        "*   **Class Weights Adjustment:** Applying more weight to under-represented classes and less weight to over-represented classes can help mitigate the effects of class imbalance.\n",
        "\n",
        "These approaches should be considered after evaluating the performance of a baseline model to determine the most effective method for balancing the dataset and improving model accuracy.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qPfBdDX_URdD"
      },
      "source": [
        "## **7. Prepare data for training**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The current dataset consists of image file names and labels stored as strings. For training a deep learning model, the data must be transformed into a format that can be processed by the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Preparing Images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cfcyM4C-CGR1"
      },
      "source": [
        "In preparing the data for training, I started by ensuring uniformity across images, setting the dimensions to 224x224, which is commonly used in pre-trained models like ResNet and VGG. This decision is rooted in literature, as standardizing image size allows for better compatibility with convolutional neural networks (CNNs), which expect fixed-size inputs. Moreover, keeping a consistent image resolution minimizes computation during resizing operations while ensuring no data is lost from inconsistent dimensions. Setting a random seed ensures the reproducibility of results, a best practice in machine learning to maintain consistency across different runs, and avoid variability in model performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YNRbcMczX5Zi"
      },
      "outputs": [],
      "source": [
        "# Define the image size for uniformity and compatibility with pre-trained models\n",
        "IMAGE_WIDTH = 224\n",
        "IMAGE_HEIGHT = 224\n",
        "RANDOM_SEED = 42"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "I added a FilePath column to store the absolute paths for the training and future datasets. This step simplifies access to images during preprocessing and training. The file path construction is essential for image loading in TensorFlow's tf.data pipeline, which ensures efficient data handling."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "laWPDQT4Yy6n"
      },
      "outputs": [],
      "source": [
        "# Directory containing the images\n",
        "image_dir = './Images/'\n",
        "\n",
        "# Add a column 'FilePath' to store the full file paths of the images for both train and future data\n",
        "train_data['FilePath'] = train_data['FileName'].apply(lambda filename: os.path.join(image_dir, filename))\n",
        "future_data['FilePath'] = future_data['FileName'].apply(lambda filename: os.path.join(image_dir, filename))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Encoding Class Labels and Binary Labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To prepare the target labels, the class labels were factorized to convert them into integers, which is required by the neural network's categorical output layer. Additionally, the MoreThanOnePerson column was converted into binary values (1 for \"YES\" and 0 for \"NO\"). This binary label is crucial for the secondary output of the model, which is designed to predict whether multiple people are present in the image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qsN9kPPBZW65"
      },
      "outputs": [],
      "source": [
        "# Encode the 'Class' column\n",
        "train_data['Class'], class_mapping = pd.factorize(train_data['Class'])\n",
        "\n",
        "# Convert 'MoreThanOnePerson' column to binary values (1 for 'YES' and 0 for 'NO')\n",
        "train_data['MoreThanOnePerson'] = train_data['MoreThanOnePerson'].apply(lambda x: 1 if x == 'YES' else 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4yAVBtykZjwp"
      },
      "outputs": [],
      "source": [
        "# Create a mapping for 'MoreThanOnePerson'\n",
        "binary_mapping = {1: 'YES', 0: 'NO'}\n",
        "\n",
        "# Save class mapping (index to class name)\n",
        "class_mapping = {index: label for index, label in enumerate(class_mapping)}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Preprocessing Image Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MyhbOb_oZyjQ"
      },
      "outputs": [],
      "source": [
        "def preprocess_image(path, lbl=None):\n",
        "    \"\"\"\n",
        "    Loads and preprocesses an image from a given file path.\n",
        "    \n",
        "    Args:\n",
        "        path (tf.Tensor): Path to the image file.\n",
        "        lbl (dict, optional): Dictionary containing labels. Defaults to None.\n",
        "    \n",
        "    Returns:\n",
        "        tf.Tensor or tuple: Preprocessed image, and optionally labels.\n",
        "    \"\"\"\n",
        "    img = tf.io.read_file(path)\n",
        "    img = tf.image.decode_jpeg(img, channels=3)\n",
        "    img = tf.image.resize(img, [IMAGE_WIDTH, IMAGE_HEIGHT])\n",
        "    img = tf.cast(img, tf.float32) / 255.0  # Normalize to [0,1]\n",
        "    \n",
        "    if lbl is not None:\n",
        "        return img, lbl\n",
        "    else:\n",
        "        return img"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A stratified split was used to partition the dataset into training and testing sets, ensuring that the class distribution remains balanced across both sets. Stratified sampling is important because it ensures that each class is proportionally represented, preventing bias in the model toward the majority class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pn8cetX4aDUQ"
      },
      "outputs": [],
      "source": [
        "# Split the dataset into training and testing sets\n",
        "train_set, test_set = train_test_split(\n",
        "    train_data,\n",
        "    test_size=0.3,\n",
        "    random_state=RANDOM_SEED,\n",
        "    stratify=train_data['Class']\n",
        ")\n",
        "\n",
        "val_set, test_set = train_test_split(\n",
        "    test_set,\n",
        "    test_size=0.5,\n",
        "    random_state=RANDOM_SEED,\n",
        "    stratify=test_set['Class']\n",
        ")\n",
        "\n",
        "print(f\"Training samples: {len(train_set)}, Testing samples: {len(test_set)}, Validation samples: {len(val_set)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For efficient data handling, a custom function was defined to create TensorFlow datasets. This function loads image file paths, applies necessary preprocessing steps such as resizing and normalization, and prepares the data for training by batching and shuffling. Using the tf.data.Dataset API is recommended for handling large datasets in TensorFlow, as it provides optimized input pipelines that support multi-threaded execution and prefetching, minimizing latency during training. The dataset is shuffled only during training to introduce randomness in each epoch, which prevents the model from learning any spurious patterns from the order of the data. Batching helps balance memory consumption and speeds up training, while prefetching ensures that the GPU is constantly fed with data without unnecessary wait times."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SxFTHUilaSHe"
      },
      "outputs": [],
      "source": [
        "# Function to create TensorFlow datasets\n",
        "def create_tf_dataset(file_paths, labels=None, shuffle_buffer=1000, batch_size=32, is_training=True):\n",
        "    \"\"\"\n",
        "    Creates a TensorFlow dataset from file paths and labels.\n",
        "    \n",
        "    Args:\n",
        "        file_paths (pd.Series): Series containing file paths.\n",
        "        labels (dict, optional): Dictionary containing labels. Defaults to None.\n",
        "        shuffle_buffer (int, optional): Buffer size for shuffling. Defaults to 1000.\n",
        "        batch_size (int, optional): Batch size. Defaults to 32.\n",
        "        is_training (bool, optional): Whether the dataset is for training. Defaults to True.\n",
        "    \n",
        "    Returns:\n",
        "        tf.data.Dataset: Prepared TensorFlow dataset.\n",
        "    \"\"\"\n",
        "    if labels:\n",
        "        dataset = tf.data.Dataset.from_tensor_slices((file_paths, labels))\n",
        "        dataset = dataset.map(lambda x, y: preprocess_image(x, y), num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    else:\n",
        "        dataset = tf.data.Dataset.from_tensor_slices(file_paths)\n",
        "        dataset = dataset.map(lambda x: preprocess_image(x), num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    \n",
        "    if is_training and labels:\n",
        "        dataset = dataset.shuffle(buffer_size=shuffle_buffer)\n",
        "    \n",
        "    dataset = dataset.batch(batch_size)\n",
        "    dataset = dataset.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "    \n",
        "    return dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The labels for both categorical (class) and binary (MoreThanOnePerson) outputs were prepared as dictionaries, making it easy to handle the multi-output nature of the model. This approach allows for the simultaneous prediction of two tasks, a strategy known as multi-task learning, which is efficient as it allows shared representation learning and can lead to better generalization. The training, validation, and future datasets were created, ensuring proper handling of both the labeled and unlabeled data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare labels as dictionaries\n",
        "train_labels = {\n",
        "    'categorical_output': train_set['Class'].values,\n",
        "    'binary_output': train_set['MoreThanOnePerson'].values\n",
        "}\n",
        "\n",
        "test_labels = {\n",
        "    'categorical_output': test_set['Class'].values,\n",
        "    'binary_output': test_set['MoreThanOnePerson'].values\n",
        "}\n",
        "\n",
        "val_labels = {\n",
        "    'categorical_output': val_set['Class'].values,\n",
        "    'binary_output': val_set['MoreThanOnePerson'].values\n",
        "}\n",
        "\n",
        "# Create TensorFlow datasets\n",
        "training_dataset = create_tf_dataset(\n",
        "    file_paths=train_set['FilePath'],\n",
        "    labels=train_labels,\n",
        "    shuffle_buffer=1000,\n",
        "    batch_size=32,\n",
        "    is_training=True\n",
        ")\n",
        "\n",
        "testing_dataset = create_tf_dataset(\n",
        "    file_paths=test_set['FilePath'],\n",
        "    labels=test_labels,\n",
        "    shuffle_buffer=0,  # No shuffling for validation\n",
        "    batch_size=32,\n",
        "    is_training=False\n",
        ")\n",
        "\n",
        "validation_dataset = create_tf_dataset(\n",
        "    file_paths=val_set['FilePath'],\n",
        "    labels=val_labels,\n",
        "    shuffle_buffer=0,  # No shuffling for validation\n",
        "    batch_size=32,\n",
        "    is_training=False\n",
        ")\n",
        "\n",
        "future_dataset = create_tf_dataset(\n",
        "    file_paths=future_data['FilePath'],\n",
        "    labels=None,\n",
        "    shuffle_buffer=0,\n",
        "    batch_size=32,\n",
        "    is_training=False\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XdhFGW2ebmm3"
      },
      "source": [
        "### Inspect dataset to check if loaded correctly"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function to visualize a batch of images and labels\n",
        "def inspect_dataset(dataset, dataset_name='Dataset'):\n",
        "    \"\"\"\n",
        "    Inspects a single batch from the dataset by printing shapes and displaying an image.\n",
        "    \n",
        "    Args:\n",
        "        dataset (tf.data.Dataset): TensorFlow dataset to inspect.\n",
        "        dataset_name (str, optional): Name of the dataset. Defaults to 'Dataset'.\n",
        "    \"\"\"\n",
        "    for batch in dataset.take(1):\n",
        "        if isinstance(batch, tuple):\n",
        "            images, labels = batch\n",
        "            print(f\"{dataset_name} - Images shape: {images.shape}\")\n",
        "            print(f\"{dataset_name} - Categorical Labels shape: {labels['categorical_output'].shape}\")\n",
        "            print(f\"{dataset_name} - Binary Labels shape: {labels['binary_output'].shape}\")\n",
        "            plt.imshow(images[0].numpy())\n",
        "            plt.title(f\"{dataset_name} - Example Image\")\n",
        "            plt.axis('off')\n",
        "            plt.show()\n",
        "        else:\n",
        "            images = batch\n",
        "            print(f\"{dataset_name} - Images shape: {images.shape}\")\n",
        "            plt.imshow(images[0].numpy())\n",
        "            plt.title(f\"{dataset_name} - Example Image\")\n",
        "            plt.axis('off')\n",
        "            plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CFuRtPGGbo9f"
      },
      "outputs": [],
      "source": [
        "# Inspect the validation dataset\n",
        "inspect_dataset(validation_dataset, dataset_name='Test Dataset')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Inspect the training dataset\n",
        "inspect_dataset(training_dataset, dataset_name='Train Dataset')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Inspect the validation dataset\n",
        "inspect_dataset(validation_dataset, dataset_name='Validation Dataset')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Inspect the future dataset\n",
        "for future_batch in future_dataset.take(1):\n",
        "    print(f\"Future Dataset - Images shape: {future_batch.shape}\")\n",
        "    plt.imshow(future_batch[0].numpy())\n",
        "    plt.title(\"Future Dataset - Example Image\")\n",
        "    plt.axis('off')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IUgiXkVfdR-6"
      },
      "source": [
        "### Create Helper functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Utility function to set up callbacks for model training\n",
        "def create_callbacks(callback_list=[]):\n",
        "    \"\"\"\n",
        "    Prepares a list of callbacks for model training, including EarlyStopping.\n",
        "    \n",
        "    Args:\n",
        "        callback_list (list, optional): List of additional callbacks to include. Defaults to an empty list.\n",
        "    \n",
        "    Returns:\n",
        "        list: A list of configured callbacks.\n",
        "    \"\"\"\n",
        "    # Initialize a list for the final callbacks\n",
        "    final_callbacks = []\n",
        "    \n",
        "    # Add any provided callbacks\n",
        "    if callback_list:\n",
        "        final_callbacks.extend(callback_list)\n",
        "    \n",
        "    # Add EarlyStopping callback with patience and best weights restoration\n",
        "    early_stop = tf.keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)\n",
        "    final_callbacks.append(early_stop)\n",
        "    \n",
        "    return final_callbacks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_training_history(history):\n",
        "    epochs = range(1, len(history.history['loss']) + 1)\n",
        "\n",
        "    # Extract metrics from history object\n",
        "    train_class_loss = history.history.get('categorical_output_loss', [])\n",
        "    val_class_loss = history.history.get('val_categorical_output_loss', [])\n",
        "    train_person_loss = history.history.get('binary_output_loss', [])\n",
        "    val_person_loss = history.history.get('val_binary_output_loss', [])\n",
        "\n",
        "    # Plot Loss for Class Output\n",
        "    plt.figure(figsize=(7, 6))\n",
        "    plt.plot(epochs, train_class_loss, 'b-', label='Training Loss (Class)')\n",
        "    plt.plot(epochs, val_class_loss, 'r-', label='Validation Loss (Class)')\n",
        "    plt.title('Training and Validation Loss (Class Output)')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    # Plot Loss for Person Output\n",
        "    plt.figure(figsize=(7, 6))\n",
        "    plt.plot(epochs, train_person_loss, 'g--', label='Training Loss (Person)')\n",
        "    plt.plot(epochs, val_person_loss, 'm--', label='Validation Loss (Person)')\n",
        "    plt.title('Training and Validation Loss (Person Output)')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def visualize_predictions(model, dataset, class_mapping, num_images=9):\n",
        "    # Create a mapping from class indices to class names\n",
        "    idx_to_class = {index: class_name for index, class_name in enumerate(class_mapping)}\n",
        "\n",
        "    # Prepare the figure for displaying images\n",
        "    plt.figure(figsize=(12, 12))\n",
        "    image_count = 0\n",
        "\n",
        "    for images, labels in dataset.take(1):  # Take one batch from the dataset\n",
        "        predictions = model.predict(images)\n",
        "\n",
        "        for i in range(min(num_images, len(images))):\n",
        "            ax = plt.subplot(3, 3, image_count + 1)\n",
        "            plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
        "            plt.axis('off')\n",
        "\n",
        "            # Get predicted class and person count\n",
        "            predicted_class = np.argmax(predictions[0][i])\n",
        "            predicted_person_label = \"YES\" if predictions[1][i] > 0.5 else \"NO\"\n",
        "\n",
        "            if labels is not None:\n",
        "                true_class = np.argmax(labels[0][i].numpy())\n",
        "                true_person_label = \"YES\" if labels[1][i].numpy() == 1 else \"NO\"\n",
        "                title = f'True: {idx_to_class[true_class]} | {true_person_label}\\nPred: {idx_to_class[predicted_class]} | {predicted_person_label}'\n",
        "            else:\n",
        "                title = f'Predicted: {idx_to_class[predicted_class]} | {predicted_person_label}'\n",
        "\n",
        "            plt.title(title)\n",
        "\n",
        "            image_count += 1\n",
        "            if image_count >= num_images:\n",
        "                break\n",
        "\n",
        "        if image_count >= num_images:\n",
        "            break\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_confusion_matrices(model, dataset, class_names):\n",
        "    actual_classes = []\n",
        "    actual_person_count = []\n",
        "    predicted_classes = []\n",
        "    predicted_person_count = []\n",
        "\n",
        "    # Iterate through the dataset to gather true and predicted labels\n",
        "    for imgs, lbls in dataset:\n",
        "        actual_classes.extend(lbls['categorical_output'].numpy())  # True class labels\n",
        "        actual_person_count.extend(lbls['binary_output'].numpy())  # True binary labels for MoreThanOnePerson\n",
        "        preds = model.predict(imgs, verbose=0)\n",
        "        predicted_classes.extend(tf.argmax(preds[0], axis=1).numpy())\n",
        "        predicted_person_count.extend([1 if p >= 0.5 else 0 for p in preds[1]])\n",
        "\n",
        "    # Create a subplot with 2 axes for two confusion matrices\n",
        "    fig, ax = plt.subplots(1, 2, figsize=(14, 8))\n",
        "\n",
        "    # Confusion matrix for class predictions\n",
        "    class_conf_matrix = confusion_matrix(actual_classes, predicted_classes)\n",
        "\n",
        "    # Generate the list of class names using the class_names mapping\n",
        "    labels = [class_names[i] for i in range(len(class_names))]\n",
        "\n",
        "    # Plot heatmap for class prediction confusion matrix\n",
        "    sns.heatmap(class_conf_matrix, annot=True, fmt='d', cmap='Blues',\n",
        "                xticklabels=labels, yticklabels=labels, ax=ax[0])\n",
        "    ax[0].set_title('Confusion Matrix - Class Predictions')\n",
        "    ax[0].set_ylabel('True Class')\n",
        "    ax[0].set_xlabel('Predicted Class')\n",
        "\n",
        "    # Confusion matrix for MoreThanOnePerson predictions\n",
        "    person_conf_matrix = confusion_matrix(actual_person_count, predicted_person_count)\n",
        "    \n",
        "    # Plot heatmap for MoreThanOnePerson prediction confusion matrix\n",
        "    sns.heatmap(person_conf_matrix, annot=True, fmt='d', cmap='Blues',\n",
        "                xticklabels=[\"NO\", \"YES\"], yticklabels=[\"NO\", \"YES\"], ax=ax[1])\n",
        "    ax[1].set_title('Confusion Matrix - MoreThanOnePerson Predictions')\n",
        "    ax[1].set_ylabel('True Label')\n",
        "    ax[1].set_xlabel('Predicted Label')\n",
        "\n",
        "    # Ensure layout is neat\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Create Baseline Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vw7mFCkudVhH"
      },
      "outputs": [],
      "source": [
        "def build_baseline_model(units=[16, 32, 64, 64], num_classes=40):\n",
        "    # Input layer\n",
        "    inputs = tf.keras.layers.Input(shape=(IMAGE_WIDTH, IMAGE_HEIGHT, 3))\n",
        "\n",
        "    # Convolutional layers with max pooling\n",
        "    x = tf.keras.layers.Conv2D(units[0], (3, 3), activation='relu')(inputs)\n",
        "    x = tf.keras.layers.MaxPooling2D((2, 2))(x)\n",
        "\n",
        "    x = tf.keras.layers.Conv2D(units[1], (3, 3), activation='relu')(x)\n",
        "    x = tf.keras.layers.MaxPooling2D((2, 2))(x)\n",
        "\n",
        "    x = tf.keras.layers.Conv2D(units[2], (3, 3), activation='relu')(x)\n",
        "    x = tf.keras.layers.MaxPooling2D((2, 2))(x)\n",
        "\n",
        "    # Flatten layer\n",
        "    x = tf.keras.layers.Flatten()(x)\n",
        "\n",
        "    # Fully connected layer\n",
        "    x = tf.keras.layers.Dense(units[3], activation='relu')(x)\n",
        "\n",
        "    # Output layers\n",
        "    class_output = tf.keras.layers.Dense(num_classes, activation='softmax', name='categorical_output')(x)  # Class output\n",
        "    person_output = tf.keras.layers.Dense(1, activation='sigmoid', name='binary_output')(x)  # Person output\n",
        "\n",
        "    # Create and return the model\n",
        "    model = tf.keras.models.Model(inputs=inputs, outputs=[class_output, person_output])\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 545
        },
        "id": "yRvUQddHeDOV",
        "outputId": "71cd9b1e-7a54-4d0a-ed1c-3044533b07a5"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mModel: \"functional\"\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)              </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">        Param # </span>┃<span style=\"font-weight: bold\"> Connected to           </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">222</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">222</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)   │            <span style=\"color: #00af00; text-decoration-color: #00af00\">448</span> │ input_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ max_pooling2d             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">111</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">111</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)   │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv2d[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)            │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">109</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">109</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">4,640</span> │ max_pooling2d[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ max_pooling2d_1           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">54</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">54</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv2d_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)            │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">52</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">52</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │         <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │ max_pooling2d_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ max_pooling2d_2           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">26</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">26</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv2d_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)            │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">43264</span>)          │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ max_pooling2d_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │      <span style=\"color: #00af00; text-decoration-color: #00af00\">2,768,960</span> │ flatten[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ class_output (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>)             │          <span style=\"color: #00af00; text-decoration-color: #00af00\">2,600</span> │ dense[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ person_output (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │             <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span> │ dense[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n",
              "└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n",
              "</pre>\n"
            ],
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m       Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to          \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ -                      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv2d (\u001b[38;5;33mConv2D\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m222\u001b[0m, \u001b[38;5;34m222\u001b[0m, \u001b[38;5;34m16\u001b[0m)   │            \u001b[38;5;34m448\u001b[0m │ input_layer[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ max_pooling2d             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m111\u001b[0m, \u001b[38;5;34m111\u001b[0m, \u001b[38;5;34m16\u001b[0m)   │              \u001b[38;5;34m0\u001b[0m │ conv2d[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           │\n",
              "│ (\u001b[38;5;33mMaxPooling2D\u001b[0m)            │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m109\u001b[0m, \u001b[38;5;34m109\u001b[0m, \u001b[38;5;34m32\u001b[0m)   │          \u001b[38;5;34m4,640\u001b[0m │ max_pooling2d[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ max_pooling2d_1           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m54\u001b[0m, \u001b[38;5;34m54\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │              \u001b[38;5;34m0\u001b[0m │ conv2d_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n",
              "│ (\u001b[38;5;33mMaxPooling2D\u001b[0m)            │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv2d_2 (\u001b[38;5;33mConv2D\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m52\u001b[0m, \u001b[38;5;34m52\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │         \u001b[38;5;34m18,496\u001b[0m │ max_pooling2d_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ max_pooling2d_2           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m26\u001b[0m, \u001b[38;5;34m26\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │              \u001b[38;5;34m0\u001b[0m │ conv2d_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n",
              "│ (\u001b[38;5;33mMaxPooling2D\u001b[0m)            │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m43264\u001b[0m)          │              \u001b[38;5;34m0\u001b[0m │ max_pooling2d_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │      \u001b[38;5;34m2,768,960\u001b[0m │ flatten[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ class_output (\u001b[38;5;33mDense\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m40\u001b[0m)             │          \u001b[38;5;34m2,600\u001b[0m │ dense[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ person_output (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │             \u001b[38;5;34m65\u001b[0m │ dense[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n",
              "└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,795,209</span> (10.66 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,795,209\u001b[0m (10.66 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,795,209</span> (10.66 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,795,209\u001b[0m (10.66 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Create the baseline model\n",
        "baseline_model = build_baseline_model()\n",
        "\n",
        "# Compile the model\n",
        "baseline_model.compile(\n",
        "    optimizer='adam',\n",
        "    loss={\n",
        "        'categorical_output': 'sparse_categorical_crossentropy',  # Loss function for class classification\n",
        "        'binary_output': 'binary_crossentropy'       # Loss function for binary classification\n",
        "    },\n",
        "    metrics={\n",
        "        'categorical_output': 'accuracy',  # Metric for class classification\n",
        "        'binary_output': tf.keras.metrics.Precision(name='precision')   # Metric for binary classification\n",
        "    }\n",
        ")\n",
        "\n",
        "# Display the model summary\n",
        "baseline_model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lRS6WcFlePzP"
      },
      "source": [
        "### Train Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YNbF0uBQeze_"
      },
      "outputs": [],
      "source": [
        "# Create a ReduceLROnPlateau callback to adjust the learning rate\n",
        "lr_scheduler_callback = tf.keras.callbacks.ReduceLROnPlateau(\n",
        "    factor=0.5,            # Reduce learning rate by a factor of 0.5\n",
        "    patience=3            # Number of epochs with no improvement to wait before reducing the learning rate\n",
        ")\n",
        "\n",
        "# Retrieve additional callbacks\n",
        "additional_callbacks = create_callbacks([lr_scheduler_callback])\n",
        "\n",
        "# Train the model with the specified callbacks and validation data\n",
        "training_history = baseline_model.fit(\n",
        "    training_dataset,\n",
        "    epochs=10,\n",
        "    callbacks=additional_callbacks,\n",
        "    validation_data=validation_dataset\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_training_history(training_history)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Model is overfitting. This is to be expected because of the small amount of data the dataset has to offered. Therefore, we will leverage transfer learning to fix the model's overfitting. Additionally, usign transfer learning will also alleviate the effects of unbalanced data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 480
        },
        "id": "PlqbqhZ1eRMb",
        "outputId": "ce77dcc6-cc9e-43e6-fd73-99a066a49ac5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m232s\u001b[0m 1s/step - class_output_accuracy: 0.0571 - loss: 4.3989 - person_output_accuracy: 0.6017 - val_class_output_accuracy: 0.0640 - val_loss: 4.2107 - val_person_output_accuracy: 0.6399 - learning_rate: 0.0010\n",
            "Epoch 2/10\n",
            "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m229s\u001b[0m 889ms/step - class_output_accuracy: 0.0750 - loss: 4.2061 - person_output_accuracy: 0.6298 - val_class_output_accuracy: 0.1220 - val_loss: 3.9098 - val_person_output_accuracy: 0.6726 - learning_rate: 0.0010\n",
            "Epoch 3/10\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-35-b27a4e877b0a>\u001b[0m in \u001b[0;36m<cell line: 24>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m# Train the model with the specified callbacks and validation data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m training_history = baseline_model.fit(\n\u001b[0m\u001b[1;32m     25\u001b[0m     \u001b[0mtraining_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[1;32m    316\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mepoch_iterator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menumerate_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 318\u001b[0;31m                     \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    319\u001b[0m                     \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pythonify_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    831\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 833\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    834\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    876\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    877\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 878\u001b[0;31m       results = tracing_compilation.call_function(\n\u001b[0m\u001b[1;32m    879\u001b[0m           \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creation_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    880\u001b[0m       )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m   \u001b[0mbound_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m   \u001b[0mflat_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbound_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m   return function._call_flat(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    140\u001b[0m       \u001b[0mflat_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m   )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1320\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1321\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1324\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;34m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m     \u001b[0mflat_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_recording\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bound_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m             outputs = self._bound_context.call_function(\n\u001b[0m\u001b[1;32m    252\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1550\u001b[0m     \u001b[0mcancellation_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcancellation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcancellation_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1552\u001b[0;31m       outputs = execute.execute(\n\u001b[0m\u001b[1;32m   1553\u001b[0m           \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1554\u001b[0m           \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     54\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "base_model = tf.keras.applications.Xception(weights='imagenet', include_top=False, input_shape=(IMAGE_WIDTH, IMAGE_HEIGHT, 3))\n",
        "\n",
        "base_model.trainable = False\n",
        "\n",
        "input_layer = tf.keras.Input(shape=(IMAGE_WIDTH, IMAGE_HEIGHT, 3))\n",
        "\n",
        "x = base_model(input_layer, training=False)\n",
        "\n",
        "x = layers.GlobalAveragePooling2D()(x)\n",
        "\n",
        "x = layers.Dense(128, activation='relu')(x)\n",
        "\n",
        "output_categorical = layers.Dense(40, activation='softmax', name='categorical_output')(x)  \n",
        "\n",
        "output_binary = layers.Dense(1, activation='sigmoid', name='binary_output')(x)\n",
        "\n",
        "pretrained = models.Model(inputs=input_layer, outputs=[output_categorical, output_binary])\n",
        "\n",
        "pretrained.compile(optimizer='adam',\n",
        "              loss={'categorical_output': 'sparse_categorical_crossentropy',\n",
        "                    'binary_output': 'binary_crossentropy'},\n",
        "              metrics={'categorical_output': 'accuracy',\n",
        "                       'binary_output': tf.keras.metrics.Precision(name='precision')})\n",
        "\n",
        "pretrained.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MKzlk-qufriZ"
      },
      "outputs": [],
      "source": [
        "history = pretrained.fit(\n",
        "    training_dataset,\n",
        "    epochs=10,\n",
        "    callbacks=create_callbacks(),\n",
        "    validation_data=validation_dataset\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "KHEeXb8Pf9h7",
        "outputId": "4ddbdd50-0f5e-4758-a54f-ec2f5935b750"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'training_history' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-36-a299d87ee36a>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplot_training_history\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_history\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'training_history' is not defined"
          ]
        }
      ],
      "source": [
        "plot_training_history(history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hma3wth4CctI"
      },
      "outputs": [],
      "source": [
        "visualize_predictions(pretrained, validation_dataset, class_mapping)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Using transfer learning where able to improve the situation somewhat, raising the validation accuracy for both outputs. However, the model is still overfitting. Therefore, we need more sophisticated methods to prevent overfitting\n",
        "\n",
        "One thing I noticed from the visualizing the predictions was the model often confuse between different classes if an image contain objects that signifies that class. For example, in an image where the main person is smoking, if in the near parameter there is an umbrella, then the model would confuse between smoking and holding an umbrella. Therefore, my intuition is having a way for the model to learn the most important part of the image for predicting the categorical class, which in this case is the main person. If we look at the image, the main class for each image can be recognized by looking at the person appears biggest in the image. Therefore, the model needs to learn how to prioritize the main person in the image.\n",
        "\n",
        "One of the ideas was proposed in this paper: https://arxiv.org/pdf/1803.10704 which introduced attention modules might help with this. These modules allow for learning of taskspecific features from the global features, whilst simultaneously allowing for features to be shared across different tasks. The architecture can be trained end-to-end and can be built upon any feed-forward neural network, is simple to implement, and is parameter efficient\n",
        "\n",
        "There are many implementations of the channel attention module, but specifically, I will use the version specified in this paper https://arxiv.org/abs/2003.09893, as the paper benchmark using the same dataset and achieved relatively good performance. Additionally, the backbone model I use for the model is the same as the one tested in the paper. Although the best model was NASNet, because of the limited computing power, I will use the smaller model but achieved high result in terms of accuracy, which is Xception"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Implement channel attention module\n",
        "def attetion_module(input_feature, ratio=8):\n",
        "    channel = input_feature.shape[-1]\n",
        "    \n",
        "    shared_1 = layers.Dense(channel // ratio, kernel_initializer='he_normal',)\n",
        "    \n",
        "    shared_2 = layers.Dense(channel, kernel_initializer='he_normal',)\n",
        "    \n",
        "    x = layers.GlobalAveragePooling2D()(input_feature)\n",
        "    \n",
        "    x = shared_1(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Activation('relu')(x)\n",
        "    \n",
        "    x = shared_2(x)\n",
        "    x = layers.Activation('relu')(x)\n",
        "    \n",
        "    return layers.Multiply()([input_feature, x])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lwwjdM9QDZrq"
      },
      "source": [
        "Similar to the model architecture mentioned in the paper: https://arxiv.org/abs/2003.09893, my model will also have the same classification section as the paper. Of course, with modifications to suit our specific problem"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bEKwKzlKDoNO"
      },
      "outputs": [],
      "source": [
        "base_model = tf.keras.applications.Xception(weights='imagenet', include_top=False, input_shape=(IMAGE_WIDTH, IMAGE_HEIGHT, 3))\n",
        "\n",
        "base_model.trainable = False\n",
        "\n",
        "input_layer = tf.keras.Input(shape=(IMAGE_WIDTH, IMAGE_HEIGHT, 3))\n",
        "\n",
        "x = base_model(input_layer, training=False)\n",
        "x = attetion_module(x)\n",
        "x = layers.GlobalAveragePooling2D()(x)\n",
        "\n",
        "x = layers.Dense(128, activation='relu')(x)\n",
        "\n",
        "x1 = layers.Dropout(0.4)(x)\n",
        "x1 = layers.Dense(64, activation='relu')(x1)\n",
        "x1 = layers.Dropout(0.4)(x1)\n",
        "x1 = layers.Dense(32, activation='relu')(x1)\n",
        "x1 = layers.Dropout(0.4)(x1)\n",
        "\n",
        "output_categorical = layers.Dense(40, activation='softmax', name='categorical_output')(x1)  \n",
        "\n",
        "x2 = layers.Dropout(0.4)(x)\n",
        "x2 = layers.Dense(64, activation='relu')(x2)\n",
        "x2 = layers.Dropout(0.4)(x2)\n",
        "x2 = layers.Dense(32, activation='relu')(x2)\n",
        "x2 = layers.Dropout(0.4)(x2)\n",
        "output_binary = layers.Dense(1, activation='sigmoid', name='binary_output')(x2)\n",
        "\n",
        "model_with_attention = models.Model(inputs=input_layer, outputs=[output_categorical, output_binary])\n",
        "\n",
        "model_with_attention.compile(optimizer='adam',\n",
        "              loss={'categorical_output': 'sparse_categorical_crossentropy',\n",
        "                    'binary_output': 'binary_crossentropy'},\n",
        "              metrics={'categorical_output': 'accuracy',\n",
        "                       'binary_output': tf.keras.metrics.Precision(name='precision')})\n",
        "\n",
        "model_with_attention.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JGgU3017EgbC"
      },
      "outputs": [],
      "source": [
        "history = model_with_attention.fit(\n",
        "    training_dataset,\n",
        "    epochs=100,\n",
        "    callbacks=create_callbacks(),\n",
        "    validation_data=validation_dataset\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "74ZYkKPoEh1w"
      },
      "outputs": [],
      "source": [
        "plot_training_history(history)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Using the attention module, the model was able to converge a good result faster. Additionally, the model was able to reduce overfitting, although the second output is still overfitting. Therefore, to reduce overfitting, we will reduce model's complexity. Specifically, I will reduce the complexity in the second branch, as the first branch is not overfitting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Reduce Overfitting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HnxtADtDE_K4"
      },
      "outputs": [],
      "source": [
        "base_model = tf.keras.applications.Xception(weights='imagenet', include_top=False, input_shape=(IMAGE_WIDTH, IMAGE_HEIGHT, 3))\n",
        "\n",
        "base_model.trainable = False\n",
        "\n",
        "input_layer = tf.keras.Input(shape=(IMAGE_WIDTH, IMAGE_HEIGHT, 3))\n",
        "\n",
        "x = base_model(input_layer, training=False)\n",
        "x = attetion_module(x)\n",
        "x = layers.GlobalAveragePooling2D()(x)\n",
        "\n",
        "x = layers.Dense(128, activation='relu')(x)\n",
        "\n",
        "x1 = layers.Dropout(0.4)(x)\n",
        "x1 = layers.Dense(64, activation='relu')(x1)\n",
        "x1 = layers.Dropout(0.4)(x1)\n",
        "x1 = layers.Dense(32, activation='relu')(x1)\n",
        "x1 = layers.Dropout(0.4)(x1)\n",
        "\n",
        "output_categorical = layers.Dense(40, activation='softmax', name='categorical_output')(x1)  \n",
        "\n",
        "x2 = layers.Dropout(0.4)(x)\n",
        "x2 = layers.Dense(32, activation='relu')(x2)\n",
        "x2 = layers.Dropout(0.4)(x2)\n",
        "x2 = layers.Dense(16, activation='relu')(x2)\n",
        "x2 = layers.Dropout(0.4)(x2)\n",
        "output_binary = layers.Dense(1, activation='sigmoid', name='binary_output')(x2)\n",
        "\n",
        "model_1 = models.Model(inputs=input_layer, outputs=[output_categorical, output_binary])\n",
        "\n",
        "model_1.compile(optimizer='adam',\n",
        "              loss={'categorical_output': 'sparse_categorical_crossentropy',\n",
        "                    'binary_output': 'binary_crossentropy'},\n",
        "              metrics={'categorical_output': 'accuracy',\n",
        "                       'binary_output': tf.keras.metrics.Precision(name='precision')})\n",
        "\n",
        "model_1.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CwyUeIUlF0S0"
      },
      "outputs": [],
      "source": [
        "history = model_1.fit(\n",
        "    training_dataset,\n",
        "    epochs=100,\n",
        "    callbacks=create_callbacks(),\n",
        "    validation_data=validation_dataset\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Atrous Convolution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "One thing I noticed is the while the first output accuracy were increasing, the second output accuracy stopped growing after it reached a value. My rationale is the feature required by the second output is different from the first output. Since the second output is predicting if an image has more than one person, it would need to look at larger view of the image, which is not the same case for the first output. The first output needs to predict the classes of the dominant person in the image, therefore it will need to look at a smaller view of the image. Thus, I will use the channel attention module on the first branch only, while for the second output will have atrous convolution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W5KKWhN_F8y4"
      },
      "outputs": [],
      "source": [
        "def atrous_conv_layer(inputs, filters, kernel_size, rate):\n",
        "    return tf.keras.layers.Conv2D(filters=filters, kernel_size=kernel_size, \n",
        "    dilation_rate=rate, padding='same', activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01))(inputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "base_model = tf.keras.applications.Xception(weights='imagenet', include_top=False, input_shape=(IMAGE_WIDTH, IMAGE_HEIGHT, 3))\n",
        "\n",
        "base_model.trainable = False\n",
        "\n",
        "input_layer = tf.keras.Input(shape=(IMAGE_WIDTH, IMAGE_HEIGHT, 3))\n",
        "\n",
        "x = base_model(input_layer, training=False)\n",
        "\n",
        "x1 = attetion_module(x)\n",
        "x1 = layers.GlobalAveragePooling2D()(x)\n",
        "x1 = layers.Dense(128, activation='relu')(x)\n",
        "x1 = layers.Dropout(0.4)(x)\n",
        "x1 = layers.Dense(64, activation='relu')(x1)\n",
        "x1 = layers.Dropout(0.4)(x1)\n",
        "x1 = layers.Dense(32, activation='relu')(x1)\n",
        "x1 = layers.Dropout(0.4)(x1)\n",
        "\n",
        "output_categorical = layers.Dense(40, activation='softmax', name='categorical_output')(x1)  \n",
        "\n",
        "x2 = atrous_conv_layer(x, 32, (7, 7), rate=2)\n",
        "x2 = atrous_conv_layer(x2, 32, (7, 7), rate=4)\n",
        "x2 = layers.GlobalAveragePooling2D()(x2)\n",
        "x2 = layers.Dropout(0.4)(x)\n",
        "x2 = layers.Dense(32, activation='relu')(x2)\n",
        "x2 = layers.Dropout(0.4)(x2)\n",
        "x2 = layers.Dense(16, activation='relu')(x2)\n",
        "x2 = layers.Dropout(0.4)(x2)\n",
        "output_binary = layers.Dense(1, activation='sigmoid', name='binary_output')(x2)\n",
        "\n",
        "model_2 = models.Model(inputs=input_layer, outputs=[output_categorical, output_binary])\n",
        "\n",
        "model_2.compile(optimizer='adam',\n",
        "              loss={'categorical_output': 'sparse_categorical_crossentropy',\n",
        "                    'binary_output': 'binary_crossentropy'},\n",
        "              metrics={'categorical_output': 'accuracy',\n",
        "                       'binary_output': tf.keras.metrics.Precision(name='precision')})\n",
        "\n",
        "model_2.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "history = model_2.fit(\n",
        "    training_dataset,\n",
        "    epochs=100,\n",
        "    callbacks=create_callbacks(),\n",
        "    validation_data=validation_dataset\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_training_history(history)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Reduce Overfitting if still exists TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **.Reference List**"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
