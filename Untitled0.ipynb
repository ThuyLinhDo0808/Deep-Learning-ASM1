{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HT2bZvmULIHO"
      },
      "source": [
        "# **A. Introduction**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O3Jmy8pi2S7y"
      },
      "source": [
        "# **B. Approach**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l9mwxdvX2wvK"
      },
      "source": [
        "\n",
        "\n",
        "## **1.   Setup Framework**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VPHuJlbl3Hg-"
      },
      "source": [
        "Before model training, it is essential to set up a robust evaluation framework to ensure that the model's performance is fairly and accurately assessed. This involves splitting the data into training, validation, and testing datasets. The training set is used to train the model, the validation set is used to tune hyperparameters, and the test set provides an unbiased evaluation of the model's performance.\n",
        "\n",
        "**Rationale:** A typical split of 70% training, 15% validation, and 15% testing ensures sufficient data for training while preserving enough data to validate the model and assess generalization performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "CeHfsOjR4LJg"
      },
      "outputs": [],
      "source": [
        "%pip install -q -U keras-tuner"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "Mg5znjZBzuvp"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import zipfile\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import keras_tuner as kt\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import seaborn as sns\n",
        "import imageio\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from pathlib import Path\n",
        "from time import strftime\n",
        "import uuid\n",
        "from tensorflow.keras import layers, models, optimizers\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.applications import Xception, MobileNetV2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9HC-qWGWztyD",
        "outputId": "2dbef011-3800-456d-fc41-2d5a98e07aaa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The tensorboard extension is already loaded. To reload it, use:\n",
            "  %reload_ext tensorboard\n"
          ]
        }
      ],
      "source": [
        "%load_ext tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "mcp97DNRzw9p"
      },
      "outputs": [],
      "source": [
        "%reload_ext tensorboard"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vGwj9U68bHgt"
      },
      "source": [
        "## **2. Connect data file**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LX4Cf50OPZxt"
      },
      "source": [
        "*   ***Mounting Google Drive:*** The code uses drive.mount('/content/drive') to mount Google Drive into the Colab environment, enabling access to files stored in the cloud. This is essential for handling large datasets and persistent storage in a seamless and reliable way, commonly used in Colab for large-scale data analysis.\n",
        "\n",
        "*   ***Setting the Data Path:*** The data_path variable specifies the location of the dataset in Google Drive. This makes the code more flexible and easier to maintain, as changes to the data location can be made by simply updating this variable.\n",
        "\n",
        "*   ***Changing the Working Directory:*** Using os.chdir(data_path), the working directory is set to the specified data path. This allows for easier file handling by making all subsequent operations relative to this directory, reducing the need for full path specifications and minimizing errors.\n",
        "\n",
        "*   ***Listing the Contents of the Current Directory:*** The command os.listdir() lists the files in the current directory, confirming that the working directory is set correctly and that the necessary data files are present. This step acts as a checkpoint before proceeding with further data processing.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O2OxOdpsbV-h",
        "outputId": "e2ab8dca-65be-4cde-cc5c-96397b34db7d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "['s1234567_predictions.csv',\n",
              " 'future_data_2024.csv',\n",
              " 'train_data_2024.csv',\n",
              " 'Images.zip',\n",
              " '__MACOSX',\n",
              " 'Images',\n",
              " 'my_logs',\n",
              " 'models',\n",
              " 'class_mapping.csv']"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "data_path = '/content/drive/MyDrive/COSC2779/s3927777-Assignment1/A1_2024_data'\n",
        "\n",
        "# Change the working directory to data_path\n",
        "os.chdir(data_path)\n",
        "\n",
        "# Print out the current directory to validate if it is the correct directory\n",
        "os.listdir()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bmqPhmgnq8WO",
        "outputId": "f0836c48-1a74-4e0e-adad-72bc48fbd5b0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "replace Images/Img_5811.jpg? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ]
        }
      ],
      "source": [
        "# Unzip image file (only if the image file hasn't been unzipped)\n",
        "!unzip -q Images.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Q78GTzPbZY4"
      },
      "source": [
        "## **3. Load CSV files**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BmhVEWKxRosY"
      },
      "source": [
        "The code loads two CSV files into Pandas DataFrames: train_data for training and future_data for future predictions. Besides, the 'FilePath' column is added to both DataFrames to store the full path to each image file. The paths are constructed by joining a common directory (image_dir) with the individual filenames from the FileName column. By that way, constructing full paths for each image ensures that the images can be easily accessed later, facilitating image loading for training and prediction. This approach centralizes file handling and ensures consistency, reducing the likelihood of errors when accessing image files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "olXu2c11bb-J"
      },
      "outputs": [],
      "source": [
        "train_data = pd.read_csv('train_data_2024.csv')\n",
        "future_data = pd.read_csv('future_data_2024.csv')\n",
        "\n",
        "image_dir = './Images/'\n",
        "# Get all file paths of the images\n",
        "train_data['FilePath'] = train_data['FileName'].apply(lambda x: os.path.join(image_dir, x))\n",
        "future_data['FilePath'] = future_data['FileName'].apply(lambda x: os.path.join(image_dir, x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vWEr1-bgpNcB"
      },
      "outputs": [],
      "source": [
        "future_data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4vOI-ZKlpRql"
      },
      "outputs": [],
      "source": [
        "train_data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qqN0r3dHlwsk"
      },
      "outputs": [],
      "source": [
        "RANDOM_STATE = 42\n",
        "tf.random.set_seed(RANDOM_STATE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mvIrk8S8bftH"
      },
      "source": [
        "## **4. Preprocess the data**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ou-PY1N74xCQ"
      },
      "source": [
        "By examining the number of distinct values in the 'Class' column, you ensure that the dataset is correctly labeled for training. If the result deviates from 40, it may indicate missing or extra classes that could impact model performance.\n",
        "\n",
        "In addition to verifying the number of classes, it's important to note that the development data includes three labels: 'Class', 'MoreThanOnePerson', and 'HighLevelCategory', with 'FileName' being excluded from predictions. For the primary model, you only need to predict the 'Class' and 'MoreThanOnePerson' labels. The 'HighLevelCategory' label, although not necessary for the main prediction task, could be useful as an auxiliary output layer. Integrating 'HighLevelCategory' in this way might help in normalizing the model or improving its performance by providing additional contextual information."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FJAQO9cLrMwz"
      },
      "outputs": [],
      "source": [
        "# Verify if the quantity of classes aligns with the specifications by confirming that there are indeed 40 classes in total\n",
        "train_data['Class'].nunique()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jED6Yjnb5dDM"
      },
      "source": [
        "By counting the number of non-null entries in the 'FileName' column, this line provides the total count of data points available for training. This count is essential for understanding the size of your dataset and ensuring that you have enough instances to build and evaluate your model effectively."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "00O3Jht5rNnr"
      },
      "outputs": [],
      "source": [
        "# Check how many instances\n",
        "train_data['FileName'].count()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gAkudRL95f83"
      },
      "source": [
        "It is also important to note that the future_data does not contain any labels, rendering it unsuitable for evaluation purposes. As a result, the data from the train_data_2024.csv file will be used and split into training, testing, and validation datasets. Furthermore, it has been confirmed that the train data contains the correct number of unique classes, aligning with the model's requirements and ensuring that the dataset is appropriately structured for training and evaluation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iBbGmVWbz9Bf"
      },
      "source": [
        "## **5. Validate Images**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7B3eHnVGz_Ty"
      },
      "outputs": [],
      "source": [
        "# Check if any Filename overlap between 2 data file\n",
        "train_data['FileName'].isin(future_data['FileName']).sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y6jaqCaY6WU6"
      },
      "source": [
        "This count helps to detect potential data leakage, which occurs if the same images are present in both datasets. Ensuring that there are no overlapping filenames is crucial for maintaining the integrity of your training and evaluation processes, as it prevents the possibility of the model being inadvertently trained or tested on duplicate data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U3_k1OPeKhFY"
      },
      "outputs": [],
      "source": [
        "# Get the size of an image given its filepath\n",
        "def image_dimensions(filepath):\n",
        "    image = imageio.imread(filepath)\n",
        "    width, height = image.shape[1], image.shape[0]\n",
        "    return width, height\n",
        "\n",
        "# Check the sizes of all images listed in the 'FileName' column of a DataFrame\n",
        "def get_all_image_sizes(data):\n",
        "    # Initialize an empty list to store the image sizes\n",
        "    image_sizes = []\n",
        "\n",
        "    # Loop over each filename in the 'FileName' column of the data\n",
        "    for filename in data['FileName']:\n",
        "        # Construct the full path to the image file\n",
        "        image_path = os.path.join('Images', filename)\n",
        "        print(image_path)\n",
        "\n",
        "        # Get the dimensions of the image\n",
        "        dimensions = image_dimensions(image_path)\n",
        "\n",
        "        # Append the dimensions to the list\n",
        "        image_sizes.append(dimensions)\n",
        "\n",
        "    return image_sizes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CLmJGz-7LhM5"
      },
      "outputs": [],
      "source": [
        "sizes = get_all_image_sizes(train_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FvUubQm06tmj"
      },
      "source": [
        "The code snippet 'image_dimensions(filepath)' defines a function to retrieve the dimensions of an image given its file path. By using 'imageio.imread(filepath)', the function loads the image, and image.shape provides its width and height. The function returns these dimensions, which are essential for analyzing and ensuring consistency in image sizes within the dataset.\n",
        "\n",
        "To check the sizes of all images listed in the 'FileName' column of a DataFrame, the function 'get_all_image_sizes(data)' is used. This function iterates over each filename, constructs the full path to the image using 'os.path.join('Images', filename)', and retrieves the image dimensions through the image_dimensions function. The sizes are then collected in a list. This approach allows you to systematically analyze the dimensions of all images in the dataset, which is crucial for preprocessing and ensuring that all images conform to the required specifications for model training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G5hjmhNjLzpN"
      },
      "outputs": [],
      "source": [
        "# Plot the sizes\n",
        "plt.hist(sizes, bins=20)\n",
        "plt.xlabel('Image Size')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Distribution of Image Sizes')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Frd__mlN7LAV"
      },
      "source": [
        "The plot illustrates the distribution of image sizes within your dataset. The following key points can be concluded:\n",
        "\n",
        "\n",
        "*   **Sparse Distribution:** The histogram shows that the image sizes are sparsely distributed across different values. This is indicated by the tall, thin lines corresponding to individual image sizes, suggesting that there are not many images of the same size.\n",
        "\n",
        "*   **No Common Image Size:** Unlike typical datasets where a peak or cluster might indicate a common or predominant image size, this dataset lacks such clustering. This could suggest significant variation in the dimensions of images.\n",
        "\n",
        "*   **Implications for Preprocessing:**\n",
        "\n",
        "  *   ***Resizing Required:*** Given the variety in image sizes, it's likely that resizing images to a uniform dimension will be necessary before feeding them into a deep learning model, especially we are planning to use a pre-trained Convolutional Neural Network (CNN).\n",
        "\n",
        "  *   ***Memory and Performance Consideration:*** The variation in image sizes might also impact memory usage and processing time, making it even more important to standardize image dimensions.\n",
        "\n",
        "\n",
        "*   **Outliers:** There might be outliers present with very large or very small dimensions compared to the rest. These should be handled carefully, as they can skew model training if not properly addressed.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oumBAqotMKUZ"
      },
      "source": [
        "## **6. Check the Imbalance of Data**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gKalpKyU8Rz1"
      },
      "source": [
        "The distribution of classes was considered, and techniques like class weighting or oversampling could be considered if significant imbalance is found."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zu1ibPsh8uBp"
      },
      "source": [
        "To analyze the distribution of occurrences within the dataset, the code snippet 'grouped_counts = train_data.groupby(['Class', 'MoreThanOnePerson']).size().reset_index(name='Count')' groups the data by the 'Class' and 'MoreThanOnePerson' columns, then counts the number of occurrences for each combination. This grouping is useful for understanding how many instances fall into each class and whether they contain more than one person.\n",
        "\n",
        "Following this, 'the pivot_table = grouped_counts.pivot(index='Class', columns='MoreThanOnePerson', values='Count').fillna(0)' reshapes the grouped data into a format suitable for plotting. By pivoting the table, you can create a stacked bar chart to visually represent the distribution of 'MoreThanOnePerson' within each 'Class'. The plotting code 'ax = pivot_table.plot(kind='bar', stacked=True, figsize=(10, 6))' generates this chart, with 'ax.set_title()', 'ax.set_xlabel()', 'ax.set_ylabel()', and 'ax.set_xticklabels()' enhancing readability and presentation. The final result is a clear visualization showing how many images in each class involve more than one person, facilitating a better understanding of the dataset's composition."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8wvbjw_iMJhm"
      },
      "outputs": [],
      "source": [
        "# Group the data and count occurrences\n",
        "grouped_counts = train_data.groupby(['Class', 'MoreThanOnePerson']).size().reset_index(name='Count')\n",
        "\n",
        "# Pivot the table to make it suitable for plotting\n",
        "pivot_table = grouped_counts.pivot(index='Class', columns='MoreThanOnePerson', values='Count').fillna(0)\n",
        "\n",
        "# Plot the data using the `plot` method\n",
        "ax = pivot_table.plot(kind='bar', stacked=True, figsize=(10, 6))  # Adjust figsize for better visualization\n",
        "ax.set_title('Distribution of MoreThanOnePerson for Each Class')\n",
        "ax.set_xlabel('Class')\n",
        "ax.set_ylabel('Count')\n",
        "ax.set_xticklabels(pivot_table.index, rotation=90)  # Rotate x-axis labels for readability\n",
        "ax.legend(title='MoreThanOnePerson')\n",
        "plt.tight_layout()  # Adjust layout for better spacing\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "62cFz-7bNQhk"
      },
      "outputs": [],
      "source": [
        "# Count occurrences of each category in 'HighLevelCategory'\n",
        "category_counts = train_data['HighLevelCategory'].value_counts()\n",
        "\n",
        "# Plot the bar chart using the `plot` method\n",
        "ax = category_counts.plot(kind='bar', figsize=(8, 5))  # Adjust figsize for visualization\n",
        "ax.set_title('Value Counts of HighLevelCategory')\n",
        "ax.set_xlabel('HighLevelCategory')\n",
        "ax.set_ylabel('Count')\n",
        "plt.xticks(rotation=45)  # Rotate x-axis labels for better readability if needed\n",
        "plt.tight_layout()  # Adjust layout to prevent overlap\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nd2vme8CAGkU"
      },
      "source": [
        "**Conclusion of the plots**\n",
        "\n",
        "The imbalance in the MoreThanOnePerson occurrences and the uneven distribution of classes and categories might affect the model’s ability to generalize across different scenarios. This imbalance could be due to varying scene complexities or contexts within each class. To address these issues, several strategies might be employed:\n",
        "\n",
        "*   **Data Augmentation:** Implementing data augmentation techniques can help balance the occurrences within each class while maintaining a balanced distribution of the MoreThanOnePerson label.\n",
        "*   **Class Weights Adjustment:** Applying more weight to under-represented classes and less weight to over-represented classes can help mitigate the effects of class imbalance.\n",
        "\n",
        "These approaches should be considered after evaluating the performance of a baseline model to determine the most effective method for balancing the dataset and improving model accuracy.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qPfBdDX_URdD"
      },
      "source": [
        "## **7. Prepare data for training**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cfcyM4C-CGR1"
      },
      "source": [
        "We the target width and height (224x224) for the images. These values are used to resize images, ensuring that they are consistent in size, which is necessary for feeding them into a Convolutional Neural Network (CNN)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YNRbcMczX5Zi"
      },
      "outputs": [],
      "source": [
        "# Define the image size for uniformity and compatibility with pre-trained models\n",
        "IMAGE_WIDTH = 224\n",
        "IMAGE_HEIGHT = 224\n",
        "RANDOM_SEED = 42"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "laWPDQT4Yy6n"
      },
      "outputs": [],
      "source": [
        "# Directory containing the images\n",
        "image_dir = './Images/'\n",
        "\n",
        "# Add a column 'FilePath' to store the full file paths of the images for both train and future data\n",
        "train_data['FilePath'] = train_data['FileName'].apply(lambda filename: os.path.join(image_dir, filename))\n",
        "future_data['FilePath'] = future_data['FileName'].apply(lambda filename: os.path.join(image_dir, filename))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qsN9kPPBZW65"
      },
      "outputs": [],
      "source": [
        "# Currently, the Class and MoreThanOnePerson labels are strings, so we one-hot encode them after splitting\n",
        "class_encoder = LabelEncoder()\n",
        "train_data['Class'] = class_encoder.fit_transform(train_data['Class'])\n",
        "train_data['MoreThanOnePerson'] = train_data['MoreThanOnePerson'].map({'YES': 1, 'NO': 0})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4yAVBtykZjwp"
      },
      "outputs": [],
      "source": [
        "# Save the mappings\n",
        "class_mapping = dict(enumerate(class_encoder.classes_))\n",
        "binary_mapping = {1: 'YES', 0: 'NO'}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MyhbOb_oZyjQ"
      },
      "outputs": [],
      "source": [
        "def load_and_preprocess_image(image_path, label=None):\n",
        "    # Load the image from the file path\n",
        "    image = tf.io.read_file(image_path)\n",
        "    image = tf.image.decode_jpeg(image, channels=3)\n",
        "    image = tf.image.resize(image, [IMAGE_WIDTH, IMAGE_HEIGHT])\n",
        "    image = tf.image.convert_image_dtype(image, tf.float32) / 255.0\n",
        "    if label is None:\n",
        "        return image\n",
        "    else:\n",
        "        return image, label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pn8cetX4aDUQ"
      },
      "outputs": [],
      "source": [
        "# Split data\n",
        "train, test = train_test_split(train_data, test_size=0.2, random_state=RANDOM_SEED, stratify=train_data['Class'])\n",
        "len(train), len(test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qe4Q9-PZaQxW"
      },
      "outputs": [],
      "source": [
        "# Check if the distribution of the training data and testing data\n",
        "train['Class'].value_counts().plot(kind='bar')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SxFTHUilaSHe"
      },
      "outputs": [],
      "source": [
        "train_ds = tf.data.Dataset.from_tensor_slices((train['FilePath'], {'categorical_output': train['Class'], 'binary_output': train['MoreThanOnePerson']}))\n",
        "train_ds = train_ds.map(lambda x, y: load_image(x, y)).shuffle(1000).batch(32).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "\n",
        "test_ds = tf.data.Dataset.from_tensor_slices((test['FilePath'], {'categorical_output': test['Class'], 'binary_output': test['MoreThanOnePerson']}))\n",
        "test_ds = test_ds.map(lambda x, y: load_image(x, y)).batch(32).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "\n",
        "future_ds = tf.data.Dataset.from_tensor_slices((future_data['FilePath']))\n",
        "future_ds = future_ds.map(lambda x: load_image(x, None)).shuffle(1000).batch(32).prefetch(buffer_size=tf.data.AUTOTUNE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XdhFGW2ebmm3"
      },
      "source": [
        "Visualize Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CFuRtPGGbo9f"
      },
      "outputs": [],
      "source": [
        "# Validate if the images are loaded correctly\n",
        "for image, label in test_ds.take(1):\n",
        "    print(image.shape)\n",
        "    print(label['categorical_output'].shape)\n",
        "    print(label['binary_output'].shape)\n",
        "    plt.imshow(image[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for image, label in train_ds.take(1):\n",
        "    print(image.shape)\n",
        "    print(label['categorical_output'].shape)\n",
        "    print(label['binary_output'].shape)\n",
        "    plt.imshow(image[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for image in future_ds.take(1):    \n",
        "    print(image.shape)\n",
        "    plt.imshow(image[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IUgiXkVfdR-6"
      },
      "source": [
        "Baseline Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vw7mFCkudVhH"
      },
      "outputs": [],
      "source": [
        "def build_baseline_model(units=[16, 32, 64, 64], num_classes=40):\n",
        "    # Input layer\n",
        "    inputs = tf.keras.layers.Input(shape=(IMAGE_WIDTH, IMAGE_HEIGHT, 3))\n",
        "\n",
        "    # Convolutional layers with max pooling\n",
        "    x = tf.keras.layers.Conv2D(units[0], (3, 3), activation='relu')(inputs)\n",
        "    x = tf.keras.layers.MaxPooling2D((2, 2))(x)\n",
        "\n",
        "    x = tf.keras.layers.Conv2D(units[1], (3, 3), activation='relu')(x)\n",
        "    x = tf.keras.layers.MaxPooling2D((2, 2))(x)\n",
        "\n",
        "    x = tf.keras.layers.Conv2D(units[2], (3, 3), activation='relu')(x)\n",
        "    x = tf.keras.layers.MaxPooling2D((2, 2))(x)\n",
        "\n",
        "    # Flatten layer\n",
        "    x = tf.keras.layers.Flatten()(x)\n",
        "\n",
        "    # Fully connected layer\n",
        "    x = tf.keras.layers.Dense(units[3], activation='relu')(x)\n",
        "\n",
        "    # Output layers\n",
        "    class_output = tf.keras.layers.Dense(num_classes, activation='softmax', name='class_output')(x)  # Class output\n",
        "    person_output = tf.keras.layers.Dense(1, activation='sigmoid', name='person_output')(x)  # Person output\n",
        "\n",
        "    # Create and return the model\n",
        "    model = tf.keras.models.Model(inputs=inputs, outputs=[class_output, person_output])\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 545
        },
        "id": "yRvUQddHeDOV",
        "outputId": "71cd9b1e-7a54-4d0a-ed1c-3044533b07a5"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mModel: \"functional\"\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)              </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">        Param # </span>┃<span style=\"font-weight: bold\"> Connected to           </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">222</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">222</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)   │            <span style=\"color: #00af00; text-decoration-color: #00af00\">448</span> │ input_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ max_pooling2d             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">111</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">111</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)   │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv2d[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)            │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">109</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">109</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">4,640</span> │ max_pooling2d[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ max_pooling2d_1           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">54</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">54</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv2d_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)            │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">52</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">52</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │         <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │ max_pooling2d_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ max_pooling2d_2           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">26</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">26</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv2d_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)            │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">43264</span>)          │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ max_pooling2d_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │      <span style=\"color: #00af00; text-decoration-color: #00af00\">2,768,960</span> │ flatten[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ class_output (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>)             │          <span style=\"color: #00af00; text-decoration-color: #00af00\">2,600</span> │ dense[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ person_output (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │             <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span> │ dense[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n",
              "└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n",
              "</pre>\n"
            ],
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m       Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to          \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ -                      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv2d (\u001b[38;5;33mConv2D\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m222\u001b[0m, \u001b[38;5;34m222\u001b[0m, \u001b[38;5;34m16\u001b[0m)   │            \u001b[38;5;34m448\u001b[0m │ input_layer[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ max_pooling2d             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m111\u001b[0m, \u001b[38;5;34m111\u001b[0m, \u001b[38;5;34m16\u001b[0m)   │              \u001b[38;5;34m0\u001b[0m │ conv2d[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           │\n",
              "│ (\u001b[38;5;33mMaxPooling2D\u001b[0m)            │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m109\u001b[0m, \u001b[38;5;34m109\u001b[0m, \u001b[38;5;34m32\u001b[0m)   │          \u001b[38;5;34m4,640\u001b[0m │ max_pooling2d[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ max_pooling2d_1           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m54\u001b[0m, \u001b[38;5;34m54\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │              \u001b[38;5;34m0\u001b[0m │ conv2d_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n",
              "│ (\u001b[38;5;33mMaxPooling2D\u001b[0m)            │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv2d_2 (\u001b[38;5;33mConv2D\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m52\u001b[0m, \u001b[38;5;34m52\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │         \u001b[38;5;34m18,496\u001b[0m │ max_pooling2d_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ max_pooling2d_2           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m26\u001b[0m, \u001b[38;5;34m26\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │              \u001b[38;5;34m0\u001b[0m │ conv2d_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n",
              "│ (\u001b[38;5;33mMaxPooling2D\u001b[0m)            │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m43264\u001b[0m)          │              \u001b[38;5;34m0\u001b[0m │ max_pooling2d_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │      \u001b[38;5;34m2,768,960\u001b[0m │ flatten[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ class_output (\u001b[38;5;33mDense\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m40\u001b[0m)             │          \u001b[38;5;34m2,600\u001b[0m │ dense[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ person_output (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │             \u001b[38;5;34m65\u001b[0m │ dense[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n",
              "└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,795,209</span> (10.66 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,795,209\u001b[0m (10.66 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,795,209</span> (10.66 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,795,209\u001b[0m (10.66 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Create the baseline model\n",
        "baseline_model = build_baseline_model()\n",
        "\n",
        "# Compile the model\n",
        "baseline_model.compile(\n",
        "    optimizer='adam',\n",
        "    loss={\n",
        "        'class_output': 'categorical_crossentropy',  # Loss function for class classification\n",
        "        'person_output': 'binary_crossentropy'       # Loss function for binary classification\n",
        "    },\n",
        "    metrics={\n",
        "        'class_output': 'accuracy',  # Metric for class classification\n",
        "        'person_output': 'accuracy'   # Metric for binary classification\n",
        "    }\n",
        ")\n",
        "\n",
        "# Display the model summary\n",
        "baseline_model.summary()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lRS6WcFlePzP"
      },
      "source": [
        "Train Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YNbF0uBQeze_"
      },
      "outputs": [],
      "source": [
        "def get_run_log_directory(root_log_directory=\"my_logs\"):\n",
        "    timestamp = strftime(\"run_%Y_%m_%d-%H_%M_%S\")\n",
        "    return Path(root_log_directory) / timestamp\n",
        "\n",
        "def generate_callbacks(custom_callbacks=None):\n",
        "    callbacks_list = custom_callbacks if custom_callbacks else []\n",
        "\n",
        "    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "    log_dir = get_run_log_directory()\n",
        "    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, profile_batch=(5, 100))\n",
        "\n",
        "    callbacks_list.append(early_stopping)\n",
        "    callbacks_list.append(tensorboard_callback)\n",
        "\n",
        "    return callbacks_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 480
        },
        "id": "PlqbqhZ1eRMb",
        "outputId": "ce77dcc6-cc9e-43e6-fd73-99a066a49ac5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m232s\u001b[0m 1s/step - class_output_accuracy: 0.0571 - loss: 4.3989 - person_output_accuracy: 0.6017 - val_class_output_accuracy: 0.0640 - val_loss: 4.2107 - val_person_output_accuracy: 0.6399 - learning_rate: 0.0010\n",
            "Epoch 2/10\n",
            "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m229s\u001b[0m 889ms/step - class_output_accuracy: 0.0750 - loss: 4.2061 - person_output_accuracy: 0.6298 - val_class_output_accuracy: 0.1220 - val_loss: 3.9098 - val_person_output_accuracy: 0.6726 - learning_rate: 0.0010\n",
            "Epoch 3/10\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-35-b27a4e877b0a>\u001b[0m in \u001b[0;36m<cell line: 24>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m# Train the model with the specified callbacks and validation data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m training_history = baseline_model.fit(\n\u001b[0m\u001b[1;32m     25\u001b[0m     \u001b[0mtraining_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[1;32m    316\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mepoch_iterator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menumerate_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 318\u001b[0;31m                     \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    319\u001b[0m                     \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pythonify_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    831\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 833\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    834\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    876\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    877\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 878\u001b[0;31m       results = tracing_compilation.call_function(\n\u001b[0m\u001b[1;32m    879\u001b[0m           \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creation_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    880\u001b[0m       )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m   \u001b[0mbound_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m   \u001b[0mflat_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbound_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m   return function._call_flat(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    140\u001b[0m       \u001b[0mflat_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m   )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1320\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1321\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1324\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;34m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m     \u001b[0mflat_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_recording\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bound_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m             outputs = self._bound_context.call_function(\n\u001b[0m\u001b[1;32m    252\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1550\u001b[0m     \u001b[0mcancellation_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcancellation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcancellation_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1552\u001b[0;31m       outputs = execute.execute(\n\u001b[0m\u001b[1;32m   1553\u001b[0m           \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1554\u001b[0m           \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     54\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Define the file path template for saving model checkpoints\n",
        "checkpoint_path = 'models/checkpoint/baseline_model_epoch_{epoch:02d}_val_loss_{val_loss:.2f}.keras'\n",
        "\n",
        "# Create a ModelCheckpoint callback to save the best model based on validation loss\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_path,\n",
        "    save_best_only=True,   # Save only the best model\n",
        "    monitor='val_loss',   # Monitor validation loss for the best model\n",
        "    mode='min',           # Save the model with the minimum validation loss\n",
        "    save_weights_only=False,  # Save the entire model (True to save only weights)\n",
        "    verbose=0             # Suppress output messages\n",
        ")\n",
        "\n",
        "# Create a ReduceLROnPlateau callback to adjust the learning rate\n",
        "lr_scheduler_callback = tf.keras.callbacks.ReduceLROnPlateau(\n",
        "    factor=0.5,            # Reduce learning rate by a factor of 0.5\n",
        "    patience=3            # Number of epochs with no improvement to wait before reducing the learning rate\n",
        ")\n",
        "\n",
        "# Retrieve additional callbacks\n",
        "additional_callbacks = generate_callbacks([lr_scheduler_callback, checkpoint_callback])\n",
        "\n",
        "# Train the model with the specified callbacks and validation data\n",
        "training_history = baseline_model.fit(\n",
        "    training_dataset,\n",
        "    epochs=10,\n",
        "    callbacks=additional_callbacks,\n",
        "    validation_data=validation_dataset\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MKzlk-qufriZ"
      },
      "outputs": [],
      "source": [
        "def plot_training_history(history):\n",
        "    # Extract metrics from history object\n",
        "    epochs = range(1, len(history.history['loss']) + 1)\n",
        "\n",
        "    train_loss = history.history.get('loss', [])\n",
        "    val_loss = history.history.get('val_loss', [])\n",
        "    train_class_accuracy = history.history.get('class_output_accuracy', [])\n",
        "    val_class_accuracy = history.history.get('val_class_output_accuracy', [])\n",
        "    train_person_accuracy = history.history.get('person_output_accuracy', [])\n",
        "    val_person_accuracy = history.history.get('val_person_output_accuracy', [])\n",
        "\n",
        "    # Create a figure with two subplots\n",
        "    plt.figure(figsize=(14, 6))\n",
        "\n",
        "    # Plot Loss\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(epochs, train_loss, 'b-', label='Training Loss')\n",
        "    plt.plot(epochs, val_loss, 'r-', label='Validation Loss')\n",
        "    plt.title('Loss')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "\n",
        "    # Plot Accuracy\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(epochs, train_class_accuracy, 'b-', label='Training Class Accuracy')\n",
        "    plt.plot(epochs, val_class_accuracy, 'r-', label='Validation Class Accuracy')\n",
        "    plt.plot(epochs, train_person_accuracy, 'g--', label='Training Person Accuracy')\n",
        "    plt.plot(epochs, val_person_accuracy, 'm--', label='Validation Person Accuracy')\n",
        "    plt.title('Accuracy')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "\n",
        "    # Adjust layout and display the plots\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "KHEeXb8Pf9h7",
        "outputId": "4ddbdd50-0f5e-4758-a54f-ec2f5935b750"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'training_history' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-36-a299d87ee36a>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplot_training_history\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_history\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'training_history' is not defined"
          ]
        }
      ],
      "source": [
        "plot_training_history(training_history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hma3wth4CctI"
      },
      "outputs": [],
      "source": [
        "# Load the model from the specified checkpoint\n",
        "baseline_model = tf.keras.models.load_model(checkpoint_path)\n",
        "\n",
        "# Continue training with the loaded model\n",
        "history = baseline_model.fit(\n",
        "    training_dataset,\n",
        "    epochs=10,\n",
        "    callbacks=cbs,\n",
        "    validation_data=validation_dataset\n",
        ")\n",
        "\n",
        "# Save the best model after additional training\n",
        "baseline_model.save('baseline_model.keras')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lwwjdM9QDZrq"
      },
      "source": [
        "Evaluate the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bEKwKzlKDoNO"
      },
      "outputs": [],
      "source": [
        "# Load the best performing model\n",
        "baseline_model = tf.keras.models.load_model('baseline_model.keras')\n",
        "\n",
        "# Evaluate the model on the test dataset\n",
        "evaluation_results = baseline_model.evaluate(testing_dataset)\n",
        "\n",
        "# Print the evaluation results\n",
        "print(\"Evaluation Results:\")\n",
        "for metric_name, metric_value in zip(baseline_model.metrics_names, evaluation_results):\n",
        "    print(f\"{metric_name}: {metric_value:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JGgU3017EgbC"
      },
      "outputs": [],
      "source": [
        "def visualize_predictions(model, dataset, class_mapping, num_images=5):\n",
        "    # Take a random batch from the dataset\n",
        "    sample = dataset.take(1)\n",
        "\n",
        "    # Create a mapping from class indices to class names\n",
        "    idx_to_class = {index: class_name for index, class_name in enumerate(class_mapping)}\n",
        "\n",
        "    # Create a subplot grid with num_images columns\n",
        "    fig, axes = plt.subplots(1, num_images, figsize=(15, 15))\n",
        "    if num_images == 1:\n",
        "        axes = [axes]  # Ensure axes is always a list\n",
        "\n",
        "    img_count = 0\n",
        "\n",
        "    # Process the sample batch\n",
        "    for batch in sample:\n",
        "        if isinstance(batch, tuple):\n",
        "            images, labels = batch\n",
        "            labels_available = True\n",
        "        else:\n",
        "            images = batch\n",
        "            labels_available = False\n",
        "\n",
        "        for i in range(min(num_images, len(images))):\n",
        "            img = images[i].numpy()  # Convert tensor to numpy array for plotting\n",
        "            img_array = tf.expand_dims(img, axis=0)  # Add batch dimension for prediction\n",
        "            predictions = model.predict(img_array)\n",
        "\n",
        "            # Extract predictions\n",
        "            predicted_class = np.argmax(predictions[0], axis=1)[0]\n",
        "            predicted_num_of_people = predictions[1][0][0]\n",
        "            predicted_num_of_people_label = \"YES\" if predicted_num_of_people >= 0.5 else \"NO\"\n",
        "\n",
        "            # Prepare title text\n",
        "            if labels_available:\n",
        "                true_class = np.argmax(labels[0][i].numpy())\n",
        "                true_class_label = idx_to_class[true_class]\n",
        "                true_num_of_people = \"YES\" if labels[1][i].numpy() == 1 else \"NO\"\n",
        "                title_text = (f'True: {true_class_label} / {true_num_of_people}\\n'\n",
        "                              f'Predicted: {idx_to_class[predicted_class]} / {predicted_num_of_people_label}')\n",
        "            else:\n",
        "                title_text = f'Predicted: {idx_to_class[predicted_class]} / {predicted_num_of_people_label}'\n",
        "\n",
        "            # Plot the image\n",
        "            axes[i].imshow(img)\n",
        "            axes[i].set_title(title_text)\n",
        "            axes[i].axis('off')\n",
        "\n",
        "            img_count += 1\n",
        "            if img_count >= num_images:\n",
        "                break\n",
        "\n",
        "        if img_count >= num_images:\n",
        "            break\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "74ZYkKPoEh1w"
      },
      "outputs": [],
      "source": [
        "visualize_predictions(baseline_model, testing_dataset, class_mapping, num_images=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KLizHYChE_OM"
      },
      "outputs": [],
      "source": [
        "visualize_predictions(baseline_model, future_dataset, class_mapping, num_images=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HnxtADtDE_K4"
      },
      "outputs": [],
      "source": [
        "def plot_confusion_matrices(model, dataset, class_mapping):\n",
        "    # Initialize lists to store true and predicted labels\n",
        "    true_labels = []\n",
        "    true_more_than_one_person = []\n",
        "    predicted_classes = []\n",
        "    predicted_more_than_one_person = []\n",
        "\n",
        "    # Iterate over the dataset to collect true and predicted labels\n",
        "    for images, labels in dataset:\n",
        "        true_labels.extend(np.argmax(labels[0].numpy(), axis=1))  # Collect true class labels\n",
        "        true_more_than_one_person.extend(labels[1].numpy())  # Collect true \"more than one person\" labels\n",
        "\n",
        "        # Predict using the model\n",
        "        predictions = model.predict(images)\n",
        "        predicted_classes.extend(np.argmax(predictions[0], axis=1))  # Collect predicted class labels\n",
        "        predicted_more_than_one_person.extend((predictions[1] >= 0.5).astype(int).flatten())  # Collect predicted \"more than one person\" labels\n",
        "\n",
        "    # Create a figure with two subplots for the confusion matrices\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
        "\n",
        "    # Plot confusion matrix for class predictions\n",
        "    conf_matrix_class = confusion_matrix(true_labels, predicted_classes)\n",
        "    sns.heatmap(conf_matrix_class, annot=True, fmt='d', cmap='Blues',\n",
        "                xticklabels=list(class_mapping.values()), yticklabels=list(class_mapping.values()), ax=axes[0])\n",
        "    axes[0].set_title('Confusion Matrix for Class Predictions')\n",
        "    axes[0].set_ylabel('True Label')\n",
        "    axes[0].set_xlabel('Predicted Label')\n",
        "\n",
        "    # Plot confusion matrix for \"more than one person\" predictions\n",
        "    conf_matrix_person = confusion_matrix(true_more_than_one_person, predicted_more_than_one_person)\n",
        "    sns.heatmap(conf_matrix_person, annot=True, fmt='d', cmap='Blues',\n",
        "                xticklabels=[\"NO\", \"YES\"], yticklabels=[\"NO\", \"YES\"], ax=axes[1])\n",
        "    axes[1].set_title('Confusion Matrix for MoreThanOnePerson Predictions')\n",
        "    axes[1].set_ylabel('True Label')\n",
        "    axes[1].set_xlabel('Predicted Label')\n",
        "\n",
        "    # Adjust layout to prevent overlap\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CwyUeIUlF0S0"
      },
      "outputs": [],
      "source": [
        "plot_confusion_matrices(baseline_model, testing_dataset, class_mapping)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W5KKWhN_F8y4"
      },
      "outputs": [],
      "source": [
        "# Save the best model\n",
        "baseline_model.save('baseline_model.keras')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
